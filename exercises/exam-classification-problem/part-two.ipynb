{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentals of Information Systems\n",
    "\n",
    "## Python Programming (for Data Science)\n",
    "\n",
    "### Master's Degree in Data Science\n",
    "\n",
    "#### Giorgio Maria Di Nunzio\n",
    "#### (Courtesy of Gabriele Tolomei FIS 2018-2019)\n",
    "<a href=\"mailto:giorgiomaria.dinunzio@unipd.it\">giorgiomaria.dinunzio@unipd.it</a><br/>\n",
    "University of Padua, Italy<br/>\n",
    "2021/2022<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 13: The Classification Problem - Example (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "-  We consider the dataset file <code>**dataset.csv**</code>, which is contained in the <code>**loan-prediction**</code> directory on the Moodle page.\n",
    "\n",
    "-  A description of the dataset is available in the <code>**README.txt**</code> file on the same directory.\n",
    "\n",
    "-  **GOAL:** Use information from past loan applicants contained in <code>**dataset.csv**</code> to predict whether a _new_ applicant should be granted a loan or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Import stats module from scipy, which contains a large number \n",
    "# of probability distributions as well as an exhaustive library of statistical functions.\n",
    "import scipy.stats as stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the local dataset file\n",
    "DATASET_PATH = \"./data/loan-prediction/dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset with Pandas\n",
    "data = pd.read_csv(DATASET_PATH, sep=\",\", index_col=\"Loan_ID\")\n",
    "print(\"Shape of the dataset: {}\".format(data.shape))\n",
    "data.head()\n",
    "# NOTE: the first line of the file is considered as the header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Missing Values (NA)\n",
    "\n",
    "Need to have the least amount of missing values. We can discard the record or the fieature. This can be harmful cuz we are removing information and dataset which could and will help us to reduce to in and out sample error.\n",
    "\n",
    "Simplest thing to do is to set the missing values to the avg value to tha specific feature. I.e. the age of a person we have a value that will be equally distant to each other value.\n",
    "\n",
    "Shortest way to creat function, lambda function. Given argument x, if the column is numeric, fill the empty value with the median value of the column. Media is better than mean cuz it is very sensitive to extreme value while mean is exactly the same. If it is not a numeric type, you can use the mode of the columnm and iloc[0]. The elemnt of a catgeircal value of the frequen element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# is_numeric_dtype(pandas.Series) returns True iff the dtype associated with the pandas.Series is numeric\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "data = data.apply(lambda x: x.fillna(x.median()) \n",
    "                      if is_numeric_dtype(x) \n",
    "                      else x.fillna(x.mode().iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Outliers\n",
    "\n",
    "Extreme values ar not necessarily a negative thing. If thei are real. In the loan prediction is like a billionaire. Sometimes we get a billione inside all the avg salaries. Will change the avgs but it is part of dataset.\n",
    "\n",
    "Another thing is whan you have extreme value but it seems to be a mistake. Alway a good wait to choose the models that are the simplest ones, thei not vary to much given extreme cases\n",
    "\n",
    "We can keep them as they are and rely on the dataset or we try to reduce the extremes, like if you think in terms of ...\n",
    "\n",
    "Don leve the transformation without comments kuz you are effectibely schrinking data, compressing them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's winsorize 'ApplicantIncome', 'CoapplicantIncome', and 'LoanAmount'\n",
    "stats.mstats.winsorize(data.ApplicantIncome, limits=0.05, inplace=True)\n",
    "stats.mstats.winsorize(data.CoapplicantIncome, limits=0.05, inplace=True)\n",
    "stats.mstats.winsorize(data.LoanAmount, limits=0.05, inplace=True)\n",
    "\n",
    "# Apply log-transformation to 'ApplicantIncome' and assign it to a new column\n",
    "data['Log_ApplicantIncome'] = data.ApplicantIncome.apply(np.log)\n",
    "# Apply log-transformation to 'LoanAmount' and assign it to a new column\n",
    "data['Log_LoanAmount'] = data.LoanAmount.apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Categorical Features: One-Hot Encoding\n",
    "\n",
    "All the models need numerical amtrix, we will do these tranformation. How to transform something not numerical to something that is. It has a name, one-hot encoding. Suppose we have a column with label 1. I assign every lable with the corrispongin number. \n",
    "\n",
    "What is the meaning of the 2d space of the 20/30 etc. What if instead of 20/30... er have 200/300. We are placing pints in a two dim space. This will a ffect the traingin, the KNN will be also affected. Corrispondance between label and a number is very diffucit cuz is diffciult to make sanes to what to do with the number. If we change the order the labels a dn mapping to numbers is complietly differe.\n",
    "\n",
    "We ar not going to do this. We are gouing to transform column2 (the one with the label), to more columns. One for every unique value. So with 4 labels, we will have 4 new columns (with 100, there will be 100 cols). To fill the column, we will set 1 for every correspondiv value, 0 otherwise. If the 1st value has label1 (and there are label 1 through 4) the column label 1 will have one, teh other three will have 0 value. \n",
    "\n",
    "In pandase, using the function get dummies. Stupid variable generated to structurize the space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In pandas we can achieve easily one-hot encoding using the 'get_dummies()' function\n",
    "categorical_features = [col for col in data.columns if not is_numeric_dtype(data[col]) and col != 'Loan_Status']\n",
    "data_with_dummies = pd.get_dummies(data, columns = categorical_features)\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop function remove from one column and returns it. Insert the elements in the last column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Just as a convention, I prefer to place the column to be predicted\n",
    "# as the last one.\n",
    "columns = data_with_dummies.columns.tolist()\n",
    "\n",
    "# Popping out 'Loan_Status' from the list and insert it back at the end.\n",
    "columns.insert(len(columns), columns.pop(columns.index('Loan_Status')))\n",
    "\n",
    "# Let's refactor the DataFrame using this new column index\n",
    "data_with_dummies = data_with_dummies.loc[:, columns]\n",
    "data_with_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Binary Class Label\n",
    "\n",
    "In this case, we are re-labeling the output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = data_with_dummies\n",
    "data.Loan_Status = data.Loan_Status.map(lambda x: 1 if x=='Y' else -1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Building a Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Splitting the Dataset: _Training_ vs. _Test_\n",
    "\n",
    "Thi line says: take all the columns except the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the feature matrix from our original DataFrame.\n",
    "\"\"\"\n",
    "# Feature matrix X is composed of all the columns \n",
    "# except 'Loan_Status' (i.e., the target class label)\n",
    "X = data.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate X and y cuz this i how sklearn works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Similarly, we want to extract the target class column vector y.\n",
    "\"\"\"\n",
    "y = data.Loan_Status\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's split our dataset with scikit-learn 'train_test_split' function, \n",
    "which splits the input dataset into training and test set, respectively.\n",
    "We want the training set to account for 80% of the original dataset, whilst \n",
    "the test set to account for the remaining 20%.\n",
    "Additionally, we would like to take advantage of stratified sampling,\n",
    "so as to obtain the same target distribution in both the training and the test sets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=43, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training Set shape: {}\".format(X_train.shape))\n",
    "print(\"Test Set shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Scaling: Why/When\n",
    "\n",
    "-  <span style=\"color: red\">**REMEMBER:**</span> Not every learning models are sensitive to different feature scales! \n",
    "\n",
    "-  For example, in the case of Logistic Regression the vector of model parameters we come up with when we minimize the negative log-likelihood - using gradient descent (iterative) solution - is **not** affected by different feature scales, except for a constant.\n",
    "\n",
    "-  You can convince yourself of this by computing the gradient of the negative log-likelihood using non-scaled and scaled features.\n",
    "\n",
    "-  Other models, instead, are not invariant with respect to scalar transformations of the input (features), and leads to completely different results if features are not properly scaled (e.g., Support Vector Machines or SVM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Scaling: How\n",
    "\n",
    "-  Feature scaling **cannot** be done looking at the whole dataset!\n",
    "\n",
    "-  In other words, either you standardize (using **z-scores**) or normalize (using **min-max**) your features you **must** do it considering only the training set portion of your dataset.\n",
    "\n",
    "-  The same scaling, then, should be applied to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's use two different feature scaling strategies: standard z-scores and min-max\n",
    "\"\"\"\n",
    "# The following is the scikit-learn package which provides\n",
    "# various preprocessing capabilities\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can scale according to a standard scaler. Is a conventient functon. FInd the min of the values of the feature, find std dev, use the formula line 7 to \n",
    "\n",
    "Ore we can use the minimax scaler, find max and min value fo the feature and than make a scale devided by the difference between max and min. Is it mandatory to scale things, no, but it may be effective. In terms of performance, the only answer is to try it and then we will see compared to model without scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Standardizing features using z-score\n",
    "std_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scaler.transform(X_train)\n",
    "# Alternatively, using pure pandas:\n",
    "# X_train_mean = X_train.mean()\n",
    "# X_train_std = X_train.std()\n",
    "# X_train_std = (X_train - X_train_mean)/X_train_std\n",
    "\n",
    "# Normalizing features using min-max\n",
    "minmax_scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train_minmax = minmax_scaler.transform(X_train)\n",
    "# Alternatively, using pure pandas:\n",
    "# X_train_max = X_train.max()\n",
    "# X_train_min = X_train.min()\n",
    "# X_train_minmax = (X_train - X_train_min)/(X_train_max - X_train_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this stage, we can work with 3 different feature matrices:\n",
    "- The original one: X_train\n",
    "- The standardized one: X_train_std\n",
    "- The min-max normalized one: X_train_minmax\n",
    "\n",
    "In the following, however, we work only on the original feature matrix X_train\n",
    "\n",
    "\n",
    "\n",
    "How are we avluating the model? Lots of perfomarmance measure in sklearn. Not gonna ask to implemnt recalla nd precision. THe thing is to make sure how these measure work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General function used to assess the quality of predictions\n",
    "in terms of two scores: accuracy and ROC AUC (Area Under the ROC Curve)\n",
    "\"\"\"\n",
    "def evaluate(true_values, predicted_values):\n",
    "    \n",
    "    # Classification Accuracy\n",
    "    print(\"Accuracy = {:.3f}\".\n",
    "          format(accuracy_score(true_values, predicted_values)))\n",
    "    \n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print(\"Area Under the ROC Curve (ROC AUC) = {:.3f}\".\n",
    "          format(roc_auc_score(true_values, predicted_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create logistic regression object\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "# 1. Try to fit this logistic regressor to our original training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Assess the quality of predictions made on the same training set\n",
    "print(\"***** Evaluate Performance on Training Set *****\")\n",
    "evaluate(y_train, model.predict(X_train))\n",
    "print()\n",
    "\n",
    "# 3. Assess the quality of predictions made on the test set\n",
    "print(\"***** Evaluate Performance on Test Set *****\") \n",
    "evaluate(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For class -1, people who get the loan, \n",
    "\n",
    "<!-- \n",
    "Il prof ha sbagliato adire te robe.\n",
    "\n",
    "Whe the model says the person will get the loan, it will be very precise, vry like a person that a preso get a loan for sure. ONly 7/100 the model makes mistake. Problem is that model says no to lot of people who wold need th loan. Very low recall for people who needs a loan.\n",
    "\n",
    "Easyear to classify to say no than yes cuz you have more datapoints about the negative class -->\n",
    "\n",
    "\n",
    "The optimization wnet to the other way aroind to usually would see. Optimize high recall form positive class. Basically saying yes to evrityingh, is like a perfect acceptor. Including a loto of potential people who do not have to get the loan. What is the situation that is more risky: better o give loan to people who shouldng or is it to risky the otherway: this is the decision theory where to put the threshold .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having just one test set an trainign set is not good. A better way is better to use the simplest cross validation. \n",
    "\n",
    "This returns result for each situation fold. IN three folds out of the the accuracy\n",
    "\n",
    "Fermormance that goes from 67 upt to 86 precent. We have 20 percent to accuracy different which s a lot. We have no idea which is better: it is a distribution of accuracies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Simplest usage of cross-validation\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "cv = cross_validate(model, X, y, cv=10, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "pd.DataFrame(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation using cross-validation\n",
    "print(\"***** Evaluate Average Performance on Training Set *****\")\n",
    "print(\"Avg. Training Set Accuracy = {:.3f}\".format(np.mean(cv['train_accuracy'])))\n",
    "print(\"Avg. Training Set ROC AUC = {:.3f}\".format(np.mean(cv['train_roc_auc'])))\n",
    "print()\n",
    "print(\"***** Evaluate Average Performance on Cross-Validation Set *****\")\n",
    "print(\"Avg. Test Set Accuracy = {:.3f}\".format(np.mean(cv['test_accuracy'])))\n",
    "print(\"Avg. Test Set ROC AUC = {:.3f}\".format(np.mean(cv['test_roc_auc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define an object of type KFold and pass it to the cross_validate function\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv = cross_validate(model, X, y, cv=k_fold, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation using cross-validation\n",
    "print(\"***** Evaluate Average Performance on Training Set *****\")\n",
    "print(\"Avg. Training Set Accuracy = {:.3f}\".format(np.mean(cv['train_accuracy'])))\n",
    "print(\"Avg. Training Set ROC AUC = {:.3f}\".format(np.mean(cv['train_roc_auc'])))\n",
    "print()\n",
    "print(\"***** Evaluate Average Performance on Cross-Validation Set *****\")\n",
    "print(\"Avg. Test Set Accuracy = {:.3f}\".format(np.mean(cv['test_accuracy'])))\n",
    "print(\"Avg. Test Set ROC AUC = {:.3f}\".format(np.mean(cv['test_roc_auc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define an object of type StratifiedKFold and pass it to the cross_validate function\n",
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "\n",
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=37)\n",
    "\n",
    "cv = cross_validate(model, X, y, cv=k_fold, scoring=('roc_auc', 'accuracy'), return_train_score=True)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation using cross-validation\n",
    "print(\"***** Evaluate Average Performance on Training Set *****\")\n",
    "print(\"Avg. Training Set Accuracy = {:.3f}\".format(np.mean(cv['train_accuracy'])))\n",
    "print(\"Avg. Training Set ROC AUC = {:.3f}\".format(np.mean(cv['train_roc_auc'])))\n",
    "print()\n",
    "print(\"***** Evaluate Average Performance on Cross-Validation Set *****\")\n",
    "print(\"Avg. Test Set Accuracy = {:.3f}\".format(np.mean(cv['test_accuracy'])))\n",
    "print(\"Avg. Test Set ROC AUC = {:.3f}\".format(np.mean(cv['test_roc_auc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Selection and Evaluation\n",
    "\n",
    "-  So far, we have just focused on a very specific _instance_ of a Logistic Regression model.\n",
    "\n",
    "-  In other words, we haven't spent time trying to _tune_ any \"meta-parameter\" (known as **hyperparameter**) of our model.\n",
    "\n",
    "-  In practice, we used default values of hyperparameters for our Logistic Regression model, according to <code>**scikit-learn**</code>\n",
    "\n",
    "-  We didn't perform any actual model selection, as hyperparameters are fixed!\n",
    "\n",
    "-  The figures we output for test accuracy/ROC AUC scores are our estimates of _generalization_ performance of our model (i.e., evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Selection and Evaluation (cont'd)\n",
    "\n",
    "-  Most of the time, though, we may need to do one of the following:\n",
    "    1.  Fix a \"family\" of models (e.g., Logistic Regression) and perform hyperparameter selection;\n",
    "    2.  Choose between a set of models (e.g., Logistic Regression, SVM, Decision Tree, etc.), each one with a fixed (i.e., default) set of hyperparameters;\n",
    "    3.  A mixture of the above, where we have to select the best hyperparameters of the best model picked from a set of different models.\n",
    "\n",
    "-  In any case, we also need to provide an estimate of the generalization performance of the chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case 1: Select Best Hyperparameters of a Fixed Family of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1: Using Validation Set\n",
    "\n",
    "Dictionary, for each model we have a listo of all the hyperparameters\n",
    "\n",
    "Logistic rgression with one fixed hypermarameter, which is the algorhtm to solve Then a dictionary to hyperparameter values for the parameter C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models_and_hyperparams = {'LogisticRegression': (LogisticRegression(solver = \"liblinear\"),\n",
    "                                                 {'C': [0.01, 0.05, 0.1, 0.5, 1, 2]}\n",
    "                                                )\n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer splitting, separates trining and testing. Test meaning th portion of the dataset that will use to produce the iutput of the perofmrnce of the model once you optimize it. \n",
    "\n",
    "Inner splitting, is just a re applicaiton of he splitting. A way to produce a kind of  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Outer splitting: Training vs. Test set (e.g., 80รท20) used to separate training-selection-evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=73, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# Inner splitting (i.e., within the outer training set): Training vs. Validation (e.g., 80รท20)\n",
    "# Training set is used to train the model, validation set is used to select the best hyperparameters\n",
    "X_train_train, X_validation, y_train_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Keep the training score obtained with each hyperparameter\n",
    "training_scores = {}\n",
    "# Keep the validation score obtained with each hyperparameter\n",
    "validation_scores = {}\n",
    "# Keep only the best training/validation scores\n",
    "best_training_score = {}\n",
    "best_validation_score = {}\n",
    "\n",
    "# Get the only model available\n",
    "model = models_and_hyperparams['LogisticRegression'][0]\n",
    "# Get the hyperparameters for that model\n",
    "hyperparams = models_and_hyperparams['LogisticRegression'][1]\n",
    "\n",
    "# Loop through all the hyperparameters\n",
    "for hp in hyperparams:\n",
    "    training_scores[hp] = {}\n",
    "    validation_scores[hp] = {}\n",
    "    \n",
    "    # Loop through all the value of a specific hyperparameter\n",
    "    for val in hyperparams[hp]:\n",
    "        # set the model's hyperparameter to the current value\n",
    "        model.set_params(**{hp: val})\n",
    "        \n",
    "        # fit the model on the inner training portion \n",
    "        model.fit(X_train_train, y_train_train)\n",
    "        \n",
    "        # store the inner training score\n",
    "        training_score = accuracy_score(y_train_train, model.predict(X_train_train))\n",
    "        training_scores[hp][val] = training_score\n",
    "        \n",
    "        # store the inner validation score\n",
    "        validation_score = accuracy_score(y_validation, model.predict(X_validation))\n",
    "        validation_scores[hp][val] = validation_score\n",
    "        \n",
    "        # Update best training/validation scores\n",
    "        if not best_training_score:\n",
    "            best_training_score[hp] = (val, training_score)\n",
    "        else:\n",
    "            if best_training_score[hp][1] < training_score:\n",
    "                best_training_score[hp] = (val, training_score)\n",
    "                \n",
    "        if not best_validation_score:\n",
    "            best_validation_score[hp] = (val, validation_score)\n",
    "        else:\n",
    "            if best_validation_score[hp][1] < validation_score:\n",
    "                best_validation_score[hp] = (val, validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Across of all possible values, the best one in the validation set is 0.5. What do we do now? Now that we have found the best value in the hyperparamenter C, we want to use the entire trining set for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"***** Evaluate Performance on Training Set *****\")\n",
    "print(training_scores)\n",
    "print(\"***** Evaluate Performance on Validation Set *****\")\n",
    "print(validation_scores)\n",
    "print(\"***** Best Accuracy Score on Training Set *****\")\n",
    "print(best_training_score)\n",
    "print(\"***** Best Accuracy Score on Validation Set *****\")\n",
    "print(best_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pick best parameter in the dictionary we have and reset the model with this hyperparameter. What do we do? Noew can train the model on the entire training ste xtrain. We can use the entire information in the trining set. Once we trained this we can truly test it. that is the piplein for an hold out istuatin. Use hluld out approach when you have reasonable amount of points. No effect random of having one portion of the dataset or the other.  In general we will use crsso validation approach. Pipleine of cross validation but need to introducte the c-valid part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We set the model's hyperparameters to those leading to the best score on the validation test\n",
    "best_params = dict([(list(best_validation_score.keys())[0], list(best_validation_score.values())[0][0])])\n",
    "model.set_params(**best_params)\n",
    "\n",
    "# We fit this model to the whole training set portion\n",
    "model.fit(X_train, y_train)\n",
    "print(\"***** Evaluate Performance on Training Set *****\")\n",
    "evaluate(y_train, model.predict(X_train))\n",
    "print(\"***** Evaluate Performance on Test Set *****\")\n",
    "evaluate(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2.a: Using Cross-Validation (Single Hyperparameter)\n",
    "\n",
    "Define same family with the same hyper parameter of before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=73, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we declare the kfold object and even for the kfold cross validaton we can do a stratifiec crss falidation. In each fold thre is the same proportion of classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models_and_hyperparams = {'LogisticRegression': (LogisticRegression(solver = \"liblinear\"),\n",
    "                                                 {'C': [0.01, 0.05, 0.1, 0.5, 1, 2]}\n",
    "                                                )\n",
    "                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Get the only model available\n",
    "model = models_and_hyperparams['LogisticRegression'][0]\n",
    "\n",
    "# Get the hyperparameters for that model\n",
    "hyperparams = models_and_hyperparams['LogisticRegression'][1]\n",
    "\n",
    "gs = GridSearchCV(estimator=model, param_grid=hyperparams, cv=k_fold, \n",
    "                  scoring='accuracy',\n",
    "                  verbose=True,\n",
    "                 return_train_score=True)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameter: {}\".format(gs.best_params_))\n",
    "print(\"Best accuracy score: {:.3f}\".format(gs.best_score_))\n",
    "evaluate(y_test, gs.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2.b: Using Cross-Validation (Multiple Hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models_and_hyperparams = {'LogisticRegression': (LogisticRegression(solver = \"liblinear\"),\n",
    "                                                 {'C': [0.01, 0.05, 0.1, 0.5, 1, 2],\n",
    "                                                 'penalty': ['l1', 'l2']}\n",
    "                                                )\n",
    "                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=31)\n",
    "\n",
    "# Get the only model available\n",
    "model = models_and_hyperparams['LogisticRegression'][0]\n",
    "\n",
    "# Get the hyperparameters for that model\n",
    "hyperparams = models_and_hyperparams['LogisticRegression'][1]\n",
    "\n",
    "gs = GridSearchCV(estimator=model, param_grid=hyperparams, cv=k_fold, \n",
    "                  scoring='accuracy',\n",
    "                  verbose=True,\n",
    "                 return_train_score=True)\n",
    "gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Best hyperparameter: {}\".format(gs.best_params_))\n",
    "print(\"Best accuracy score: {:.3f}\".format(gs.best_score_))\n",
    "evaluate(y_test, gs.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case 2: Select Best Model out of a Set of Family of Models with Fixed Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=73, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models = {'LogisticRegression': LogisticRegression(solver = \"liblinear\", max_iter=1000),\n",
    "          'LinearSVC': LinearSVC(),\n",
    "          'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "          'RandomForestClassifier': RandomForestClassifier(),\n",
    "          'GradientBoostingClassifier': GradientBoostingClassifier()\n",
    "          # Add other families of models here...\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_scores = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_scores[model_name] = cross_val_score(model, X_train, y_train, cv=k_fold, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(cv_scores).transpose()\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cv_df['avg_cv'] = np.mean(cv_df, axis=1)\n",
    "cv_df['std_cv'] = np.std(cv_df, axis=1)\n",
    "cv_df = cv_df.sort_values(['avg_cv', 'std_cv'], ascending=[False,True])\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model Selection: Logistic Regression is the best overall method, therefore we pick that!\n",
    "# Now we need to provide an estimate of its generalization performance. \n",
    "# To do so, we evaluate it against the test set portion we previously held out.\n",
    "model = models[cv_df.index[0]]\n",
    "# Re-fit the best selected model on the whole training set\n",
    "model.fit(X_train, y_train)\n",
    "# Evaluation\n",
    "print(\"***** Evaluate Performance on Training Set *****\")\n",
    "evaluate(y_train, model.predict(X_train))\n",
    "print(\"***** Evaluate Performance on Test Set *****\")\n",
    "evaluate(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case 3: Select the Best Hyperparameters AND the Best Model from a Family of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models_and_hyperparams = {'LogisticRegression': (LogisticRegression(),\n",
    "                                                 {'C': [0.01, 0.05, 0.1, 0.5, 1, 2],\n",
    "                                                 'penalty': ['l1', 'l2']}\n",
    "                                                ),\n",
    "                          'RandomForestClassifier': (RandomForestClassifier(),\n",
    "                                       {'n_estimators': [10, 50, 100]}\n",
    "                                       ),\n",
    "                          'DecisionTreeClassifier': (DecisionTreeClassifier(),\n",
    "                                                     {'criterion': ['gini', 'entropy'], \n",
    "                                                      'max_depth': [i for i in range(1, X.shape[1]+1)]}\n",
    "                                                    )\n",
    "                         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# `outer_cv` creates 10 folds for estimating generalization error\n",
    "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# when we train on a certain fold, we use a second cross-validation\n",
    "# split in order to choose hyperparameters\n",
    "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=73)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=37, \n",
    "                                                    stratify=y)\n",
    "\n",
    "# we will collect the average of the scores on the 10 outer folds in this dictionary\n",
    "# with keys given by the names of the models in `models_and_hyperparams`\n",
    "average_scores_across_outer_folds_for_each_model = dict()\n",
    "\n",
    "# find the model with the best generalization error\n",
    "for name, (model, params) in models_and_hyperparams.items():\n",
    "    # this object is a classifier that also happens to choose\n",
    "    # its hyperparameters automatically using `inner_cv`\n",
    "    model_optimizing_hyperparams = GridSearchCV(estimator=model, \n",
    "                                                param_grid=params,\n",
    "                                                cv=inner_cv, \n",
    "                                                scoring='accuracy',\n",
    "                                               verbose=True)\n",
    "\n",
    "    # estimate generalization error on the 10-fold splits of the data\n",
    "    scores_across_outer_folds = cross_val_score(model_optimizing_hyperparams,\n",
    "                                                X_train, y_train, cv=outer_cv, scoring='accuracy')\n",
    "\n",
    "    # get the mean accuracy across each of outer_cv's 10 folds\n",
    "    average_scores_across_outer_folds_for_each_model[name] = np.mean(scores_across_outer_folds)\n",
    "    performance_summary = 'Model: {name}\\nAccuracy in the 10 outer folds: {scores}.\\nAverage Accuracy: {avg}'\n",
    "    print(performance_summary.format(\n",
    "        name=name, scores=scores_across_outer_folds,\n",
    "        avg=np.mean(scores_across_outer_folds)))\n",
    "    print()\n",
    "\n",
    "print('Average score across the outer folds: ',\n",
    "      average_scores_across_outer_folds_for_each_model)\n",
    "\n",
    "many_stars = '\\n' + '*' * 100 + '\\n'\n",
    "print(many_stars + 'Now we choose the best model and refit on the whole dataset' + many_stars)\n",
    "\n",
    "best_model_name, best_model_avg_score = max(\n",
    "    average_scores_across_outer_folds_for_each_model.items(),\n",
    "    key=(lambda name_averagescore: name_averagescore[1]))\n",
    "\n",
    "# get the best model and its associated parameter grid\n",
    "best_model, best_model_params = models_and_hyperparams[best_model_name]\n",
    "\n",
    "# now we refit this best model on the whole dataset so that we can start\n",
    "# making predictions on other data, and now we have a reliable estimate of\n",
    "# this model's generalization error and we are confident this is the best model\n",
    "# among the ones we have tried\n",
    "final_model = GridSearchCV(best_model, best_model_params, cv=inner_cv)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "print('Best model: \\n\\t{}'.format(best_model), end='\\n\\n')\n",
    "print('Estimation of its generalization performance (accuracy):\\n\\t{}'.format(\n",
    "    best_model_avg_score), end='\\n\\n')\n",
    "print('Best parameter choice for this model: \\n\\t{params}'\n",
    "      '\\n(according to cross-validation `{cv}` on the whole dataset).'.format(\n",
    "      params=final_model.best_params_, cv=inner_cv))\n",
    "\n",
    "\n",
    "y_true, y_pred, y_pred_prob = y, final_model.predict(X), final_model.predict_proba(X)\n",
    "print()\n",
    "print(classification_report(y_true, y_pred))\n",
    "roc = roc_auc_score(y_true, y_pred_prob[:,1])\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy = [{:.3f}]\".format(acc))\n",
    "print(\"Area Under the ROC = [{:.3f}]\".format(roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
